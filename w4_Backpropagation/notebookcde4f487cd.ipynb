{"cells":[{"cell_type":"markdown","metadata":{},"source":["### How to apporach this assignment\n","\n","You can find all the blanks with the `### TODO ###` tag.\n","\n","#### Current Structure of the codebase\n","- Toy Dataset Generation: gen_data (checkboard, mul_circle, spiral, moon, circle, blob)\n","- Activation Functions Class: AF (linear, sigmoid, tanh, relu, softmax, leaky_relu, selu)\n","- Loss Function Class: LF (We will only implement Softmax + Cross Entropy this time)\n","- The NN Model: SimpleNN (Defined a NN with list of neuron count and activation function)\n","- Optimizer Classes: Optimizer (SGD, SGDMomentum, SGDNesterovMomentum, Adagrad, RMSprop, Adam)\n","- Training Scheduler: StopScheduler (EarlyStopScheduler)\n","- Learning Rate Scheduler: LRScheduler (ReduceLROnPlateau)\n","- Utility Functions: grouper (Group data into mini-batches)\n","- Training Loop: train_model (The main function to train a model)\n","- Animation functions for the training process: draw_process, calculate_model_boundary, draw_model_boundary, draw_metrics_history\n","    - You can skip understanding these\n","\n","#### Step 1\n","- Get an understanding of the Training Loop\n","    - We first call gen_data to get: class_cnt, training_data, validation_data, testing_data\n","    - We define our model structure and inital our SimpleNN model\n","    - We define our optimizer, lr_scheduler, stop_scheduler\n","    - We run through train_model with (model, optimizer, lr_scheduler, stop_scheduler, training_data, validation_data)\n","    - We can look at training process with draw_process\n","    - We evaluate our model through model.evaluate and look at accuracy and model boundary using draw_model_boundary\n","\n","#### Step 2\n","- Understand the SimpleNN model\n","    - `__init__` will init the weights and biases\n","    - `empty_structure` will return an empty model will all weight and biases to be zero (Will be helpful later on)\n","    - `activation_func` and `loss_func` are wrappers functions to call the real functions\n","    - `forward` and `backprop` are the forward and backward pass of the neural net\n","        - `return_activations` in the forward pass is for usage in the backward pass\n","        - We return the delta of weight and biases for this x and y in `backprop`, this will be passed to the optimzer to decide the real delta\n","    - `update_one_batch_optimizer` is for getting the average delta to the model in the mini-batch\n","        - We will pass the delta to the optimizer and let it decide how it will change the weight and biases\n","    - `loss_one` and `loss` is for returning the loss\n","    - `evaluate_one` and `evaluate` is for returning the evaluation result\n","\n","#### Step 3\n","- Make the `Basic Run` loop work by filling in some blanks\n","    - Implement forward/backward pass in SimpleNN\n","        - The forward and backward pass needs to be in vector form\n","    - Implement one of the Activation Functions (Eg. Relu)\n","        - The parameter d means derivative\n","    - Implement the Loss Function\n","        - We implement Softmax + Cross Entropy combined loss\n","    - Implement SGD to have one working optimizer\n","        - Naive SGD will just be to multiply the delta by the learning rate and update the weight and biases\n","    - Implement grouper function for making mini-batches\n","\n","#### Step 4\n","- We shall be able to run through the `Basic Run` loop and see the training process happen!\n","\n","#### Step 5\n","- Implment other Activation Functions\n","    - linear, sigmoid, tanh, relu, softmax, leaky_relu, selu\n","- Implement other Optimizers\n","    - SGDMomentum, SGDNesterovMomentum, Adagrad, RMSprop, Adam\n","- Implement other weights and biases initalization methods\n","    - Normalized Standard Normal\n","- Implment early stopping and learning rate decay\n","    - EarlyStopScheduler\n","    - ReduceLROnPlateau\n","    \n","#### Step Final\n","- Finish with the experiments detailed at the bottom\n","    - Try out different Toy Datasets\n","        - Must: \"checkboard\", \"spiral\", \"mul_circle\"\n","        - Others: \"circle\", \"moon\", \"blob\"\n","    - Try out different activation functions\n","        - Must: AF.linear, AF.sigmoid, AF.tanh, AF.relu\n","        - Others: AF.leaky_relu, AF.selu ...\n","    - Try out Different Optimizers\n","        - Must: SGD, SGD+Momentum, RMSprop, Adam\n","        - Others: SGD+NesterovMomentum, Adagrad\n","    - Try out different Gradient Descent Methods\n","        - Must: Batch, Stochastic, Mini-batch with different batch size\n","    - Try out different model structures (deep vs shallow / wide vs thin)\n","        - Must: (4)x1, (2)x4, (4)x2\n","        - Others: (4)x3 ...\n","    - Try out other weights and biases initalization methods\n","    - Try out early stopping\n","    - Try out learning rate decay"]},{"cell_type":"markdown","metadata":{},"source":["### Import Libaries\n","Since we imported numpy, you shall not need any more libaries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["from itertools import zip_longest\n","from typing import List, Tuple, Callable, Optional\n","\n","# Vector Operations\n","import numpy as np\n","\n","# For Generating Datasets\n","from sklearn.datasets import make_circles, make_moons, make_blobs, make_gaussian_quantiles\n","from sklearn.model_selection import train_test_split\n","\n","# Progress Bar\n","from tqdm.auto import tqdm\n","\n","# Plotting Diagrams\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","\n","# Plotting Animations\n","from matplotlib.animation import FuncAnimation\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Increase figure size\n","plt.rcParams[\"figure.figsize\"] = (12, 8)"]},{"cell_type":"markdown","metadata":{},"source":["### There are 6 generated datasets that you can use to try out your model\n","\n","- Must try:\n","    - checkboard: XOR like Pattern\n","    - mul_circle: class_cnt rings of Data\n","    - spiral: Spiral Shape (From CS231n)\n","- Others:\n","    - moon: Two semi-circle\n","    - circle: Two rings of Data\n","    - blob: class_cnt Guassian Distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ALL_DATA_TYPE = [\"checkboard\", \"circle\", \"mul_circle\", \"moon\", \"blob\", \"spiral\"]\n","\n","def gen_data(name, per_class_data_cnt=500, class_cnt=3, show_data=False):\n","    n_samples = per_class_data_cnt * class_cnt\n","    if name == \"circle\":\n","        class_cnt = 2\n","        X, Y = make_circles(n_samples=n_samples, noise=0.2, factor=0.3)\n","    elif name == \"mul_circle\":\n","        X, Y = make_gaussian_quantiles(n_samples=n_samples, n_features=2, n_classes=class_cnt)\n","    elif name == \"moon\":\n","        class_cnt = 2\n","        X, Y = make_moons(n_samples=n_samples, noise=0.1)\n","    elif name == \"blob\":\n","        X, Y = make_blobs(n_samples=n_samples, n_features=2, centers=class_cnt)\n","    elif name == \"checkboard\":\n","        class_cnt = 2\n","        X = np.zeros((n_samples, 2))\n","        Y = np.zeros(n_samples)\n","        offsets = [((1, 1), 0), ((1, -1), 1), ((-1, 1), 1), ((-1, -1), 0)]\n","        for bid, (offset, y) in enumerate(offsets):\n","            idx = range(per_class_data_cnt*bid//2, per_class_data_cnt*(bid+1)//2)\n","            X[idx] = (np.random.rand(per_class_data_cnt//2, 2) + 0.05) * np.array(offset)\n","            Y[idx] = y\n","    elif name == \"spiral\":\n","        X = np.zeros((n_samples, 2))\n","        Y = np.zeros(n_samples)\n","        for cid in range(class_cnt):\n","            r = np.linspace(0.0, 1, per_class_data_cnt) # radius\n","            t = np.linspace(cid*4, (cid+1)*4, per_class_data_cnt) + np.random.randn(per_class_data_cnt)*0.3 # theta\n","            idx = range(per_class_data_cnt*cid, per_class_data_cnt*(cid+1))\n","            X[idx] = np.c_[r*np.sin(t), r*np.cos(t)]\n","            Y[idx] = cid\n","    else:\n","        raise ValueError(\"Unknown Data Name!\")\n","\n","    if show_data:\n","        plt.scatter(X[:, 0], X[:, 1], c=Y, s=20)\n","        plt.show()\n","        \n","    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.25)\n","\n","    # Change index to one hot (Assumes index 0 ~ class_cnt)\n","    # [1, 2, 0, 1] -> [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]\n","    def one_hot(x, class_cnt):\n","        oh = np.zeros((len(x), class_cnt))\n","        oh[np.arange(len(x)), [int(i) for i in x]] = 1\n","        return oh.reshape(-1, class_cnt, 1)\n","\n","    training_data = list(zip(X_train.reshape(-1, 2, 1), one_hot(Y_train, class_cnt)))\n","    validation_data = list(zip(X_valid.reshape(-1, 2, 1), one_hot(Y_valid, class_cnt)))\n","    testing_data = list(zip(X_test.reshape(-1, 2, 1), one_hot(Y_test, class_cnt)))\n","    \n","    return class_cnt, training_data, validation_data, testing_data"]},{"cell_type":"markdown","metadata":{},"source":["### Demo of Spiral Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class_cnt, training_data, validation_data, testing_data = gen_data(\"spiral\", class_cnt=5, show_data=True)"]},{"cell_type":"markdown","metadata":{},"source":["### List of Activation and Loss Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Activation Functions\n","class AF():\n","    @staticmethod\n","    def linear(x, d=False):\n","        return 1 if d else x\n","\n","    @staticmethod\n","    def sigmoid(x, d=False):\n","        return np.multiply(x, 1.0 - x) if d else 1.0/(1.0 + np.exp(-x))\n","\n","    @staticmethod\n","    def tanh(x, d=False):\n","        return 1-np.multiply(x, x) if d else np.tanh(x)\n","\n","    @staticmethod\n","    def relu(x, d=False):\n","        return np.where(x > 0, 1, 0) if d else np.where(x > 0, x, 0)\n","\n","    @staticmethod\n","    def leaky_relu(x, alpha = 0.01,d=False):\n","        return np.where(x > 0, 1, alpha) if d else np.where(x > 0, x, alpha*x)\n","\n","    @staticmethod\n","    def selu(x, alpha = 0.01, d=False):\n","        return np.where(x > 0, 1 , alpha * np.exp(x)) if d else np.where(x > 0, x, alpha*(np.exp(x)-1))\n","\n","    @staticmethod\n","    def softmax(x, d=False):\n","        if d:\n","            print(\"We should not need this!\")\n","            return -1\n","        else:\n","            if len(x.shape) > 1:\n","                x_exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n","                return x_exp / np.sum(x_exp, axis=1, keepdims=True)\n","            else:\n","                x_exp = np.exp(x - np.max(x))\n","                return x_exp / np.sum(x_exp)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Loss Functions\n","class LF():\n","    @staticmethod\n","    def softmax_crossentropy(activation_output, y, d=False):\n","        if d:\n","            grad = activation_output - y\n","            return grad\n","        else:\n","            loss = -np.sum(y * np.log(activation_output + 1e-10))\n","            return loss"]},{"cell_type":"markdown","metadata":{},"source":["### The NN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SimpleNN():\n","\n","    def __init__(self, structure: List[int], af: Callable):\n","        # Save the model structure\n","        self.layer_structure = structure\n","        self.num_layers = len(structure)\n","        self.af = af\n","\n","        # Initialize the weight and bias\n","        # Weight Initialization: http://cs231n.github.io/neural-networks-2/\n","        # For weights:\n","        #   - Standard Normal: np.random.randn\n","        #   - Normalized Standard Normal: np.random.randn / np.sqrt(<previous_layer_neuron_cnt>)\n","        #   - Gaussian Normal: np.random.normal\n","        # For bias:\n","        #   - All zeros: np.zeros\n","        #   - Standard Normal: np.random.randn\n","        #   - Gaussian Normal: np.random.normal\n","        self.weights = [np.random.randn(nl, pl) for pl, nl in zip(structure[:-1], structure[1:])]\n","        self.biases = [np.zeros((l, 1)) for l in structure[1:]]\n","\n","    def empty_structure(self):\n","        # Return empty structure for weight and bias\n","        return [np.zeros(w.shape) for w in self.weights], [np.zeros(b.shape) for b in self.biases]\n","\n","    def activation_func(self, l: int, *args, **kargs):\n","        # Use softmax on last layer\n","        return AF.softmax(*args, **kargs) if l == self.num_layers-1 else self.af(*args, **kargs)\n","\n","    def loss_func(self, *args, **kargs):\n","        return LF.softmax_crossentropy(*args, **kargs)\n","\n","    def forward(self, x, return_activations=False):\n","        activations, before_activations = [x], []\n","        for i in range(self.num_layers):\n","            x = np.dot(self.weights[i], x) + self.biases[i]\n","            before_activations += [x]\n","            x = self.activation_func(i, x)\n","            activations += [x]\n","\n","        # Return value history for backprop usage\n","        return (activations, before_activations) if return_activations else activations[-1]\n","\n","    def backprop(self, x, y):\n","        # Init empty data structure to store delta\n","        delta_w, delta_b = self.empty_structure()\n","        \n","        # Feed forward pass\n","        activations, before_activations = self.forward(x, return_activations=True)\n","\n","        # Backward pass\n","        delta = self.loss_func(activations[-1], y, d=True)\n","        for i in reversed(range(self.num_layers)):\n","            delta_b[i] = delta\n","            delta_w[i] = np.dot(delta, activations[i-1].T)\n","            delta = np.dot(self.weights[i].T, delta) * self.activation_func(i-1, before_activations[i-1], d=True)\n","\n","        return delta_w, delta_b\n","        \n","    def update_one_batch_optimizer(self, mini_batch, optimizer):\n","        # Init empty data structure to store delta\n","        batch_delta_w, batch_delta_b = self.empty_structure()\n","    \n","        # Run through the batch of data\n","        for x, ohy in mini_batch:\n","            delta_w, delta_b = self.backprop(x, ohy)\n","            batch_delta_w = [bd+d for bd, d in zip(batch_delta_w, delta_w)]\n","            batch_delta_b = [bd+d for bd, d in zip(batch_delta_b, delta_b)]\n","        \n","        # Average the delta among the same batch\n","        batch_delta_w = [bd/len(mini_batch) for bd in batch_delta_w]\n","        batch_delta_b = [bd/len(mini_batch) for bd in batch_delta_b]\n","        \n","        # Change the weights and biases by using the optimizer\n","        self.weights, self.biases = optimizer.step(self.weights, self.biases, batch_delta_w, batch_delta_b)\n"," \n","    def loss_one(self, x, y):\n","        return self.loss_func(self.forward(x), y)\n","    \n","    def loss(self, evaluate_data):\n","        return sum([self.loss_one(x, y) for x, y in evaluate_data]) / len(evaluate_data)\n","\n","    def evaluate_one(self, x):\n","        return np.argmax(self.forward(x))\n","\n","    def evaluate(self, evaluate_data):\n","        evaluation_result = [(x, np.argmax(y), self.evaluate_one(x)) for x, y in evaluate_data]\n","        accuracy = sum([y==pred for x, y, pred in evaluation_result]) / len(evaluate_data)\n","        return accuracy, evaluation_result"]},{"cell_type":"markdown","metadata":{},"source":["* ### All the Optimzers\n","\n","http://cs231n.github.io/neural-networks-3/"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Optimizer():\n","    def __init__(self, empty_model_structure):\n","        # Save the empty model structure\n","        self.empty_model = empty_model_structure\n","\n","    def step(weights, biases, batch_delta_w, batch_delta_b):\n","        # Update Weight and Bias accordingly to the average of deltas of the whole batch\n","        weights = [w+bd for w, bd in zip(weights, batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, batch_delta_b)]\n","        return weights, biases\n","\n","class SGD(Optimizer):\n","    def __init__(self, empty_model_structure,\n","                 lr: float = 0.01):\n","        # Call Optimizer's init\n","        super(SGD, self).__init__(empty_model_structure)\n","        # Save Parameters\n","        self.lr = lr\n","\n","    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n","        # Multiply delta by lr\n","        ### TODO ###\n","        ret_batch_delta_w = self.lr * batch_delta_w\n","        ret_batch_delta_b = self.lr * batch_delta_b\n","        # Update Weight and Bias accordingly\n","        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n","        return weights, biases\n","\n","class SGDMomentum(Optimizer):\n","    def __init__(self, empty_model_structure,\n","                 lr: float = 0.01, momentum: float = 0.9):\n","        # Call Optimizer's init\n","        super(SGDMomentum, self).__init__(empty_model_structure)\n","        # Save Parameters\n","        self.lr = lr\n","        self.momentum = momentum\n","        # Initial momentum storage\n","        self.momentum_w, self.momentum_b = self.empty_model\n","\n","    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n","        # Multiply delta by lr and add previous batch momentum\n","        ### TODO ###\n","        ret_batch_delta_w = batch_delta_w * self.lr + self.momentum_w * self.momentum\n","        ret_batch_delta_b = batch_delta_b * self.lr + self.momentum_b * self.momentum\n","        # Save current delta for future momentum usage\n","        ### TODO ###\n","        self.momentum_w = ret_batch_delta_w\n","        self.momentum_b = ret_batch_delta_b\n","\n","        # Update Weight and Bias accordingly\n","        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n","        return weights, biases\n","\n","class SGDNesterovMomentum(Optimizer):\n","    def __init__(self, empty_model_structure,\n","                 lr: float = 0.01, momentum: float = 0.9):\n","        # Call Optimizer's init\n","        super(SGDNesterovMomentum, self).__init__(empty_model_structure)\n","        # Save Parameters\n","        self.lr = lr\n","        self.momentum = momentum\n","        # Initial momentum storage\n","        self.momentum_w, self.momentum_b = self.empty_model\n","\n","    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n","        # Save current momentum first\n","        prev_momentum_w, prev_momentum_b = self.momentum_w, self.momentum_b\n","\n","        # Multiply delta by lr and add previous batch momentum\n","        ### TODO ###\n","\n","        # Save current delta for future momentum usage\n","        ### TODO ###\n","\n","        # Modify the delta to accomplish look ahead\n","        ### TODO ###\n","        \n","        # Update Weight and Bias accordingly\n","        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n","        return weights, biases\n","\n","class Adagrad(Optimizer):\n","    EPS = 1e-6\n","    def __init__(self, empty_model_structure,\n","                 lr: float = 0.01):\n","        # Call Optimizer's init\n","        super(Adagrad, self).__init__(empty_model_structure)\n","        # Save Parameters\n","        self.lr = lr\n","        # Initial leraning rate storage\n","        self.lr_w, self.lr_b = self.empty_model\n","        \n","    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n","        # Modify learning rate for each parameter according to gradient\n","        ### TODO ###\n","\n","        # Multiply by lr with respect to learning rate of each parameter\n","        ### TODO ###\n","\n","        # Update Weight and Bias accordingly\n","        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n","        return weights, biases\n","    \n","class RMSprop(Optimizer):\n","    EPS = 1e-6\n","    def __init__(self, empty_model_structure,\n","                 lr: float = 0.01, decay_rate: float = 0.99):\n","        # Call Optimizer's init\n","        super(RMSprop, self).__init__(empty_model_structure)\n","        # Save Parameters\n","        self.lr = lr\n","        self.decay_rate = decay_rate\n","        # Initial leraning rate storage\n","        self.lr_w, self.lr_b = self.empty_model\n","\n","    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n","        # Modify learning rate for each parameter according to gradient\n","        ### TODO ###\n","\n","        # Multiply by lr with respect to learning rate of each parameter\n","        ### TODO ###\n","\n","        # Update Weight and Bias accordingly\n","        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n","        return weights, biases\n","    \n","class Adam(Optimizer):\n","    EPS = 1e-8\n","    def __init__(self, empty_model_structure,\n","                 lr: float = 0.01, beta1: float = 0.9, beta2: float = 0.999, bias_correction: bool = True):\n","        # Call Optimizer's init\n","        super(Adam, self).__init__(empty_model_structure)\n","        # Save Parameters\n","        self.lr = lr\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.bias_correction = bias_correction\n","        # Initial leraning rate and momentum storage\n","        self.lr_w, self.lr_b = self.empty_model\n","        self.momentum_w, self.momentum_b = self.empty_model\n","        if self.bias_correction:\n","            # Save the amount of steps ran\n","            self.optimize_steps = 1\n","\n","    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n","        # Modify momentum according to delta with decay\n","        ### TODO ###\n","\n","        # Modify learning rate for each parameter according to gradient\n","        ### TODO ###\n","\n","        # Multiply by lr with respect to learning rate of each parameter\n","        ### TODO ###\n","\n","        # Update Weight and Bias accordingly\n","        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n","        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n","        return weights, biases"]},{"cell_type":"markdown","metadata":{},"source":["### Early Stopping Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class StopScheduler():\n","    def __init__(self):\n","        pass\n","    \n","    def step(self, validation_loss):\n","        # Never Stop\n","        return False\n","\n","class EarlyStopScheduler(StopScheduler):\n","    def __init__(self, patience=10, threshold=1e-4):\n","        # Save Parameters\n","        self.patience = patience\n","        self.threshold = threshold\n","        # Save past loss\n","        self.past_loss = []\n","\n","    def step(self, validation_loss):\n","        # Append loss to past loss\n","        self.past_loss.append(validation_loss)\n","\n","        # Early stop if no loss improved more than threshold% in the last patience steps\n","        ### TODO ###\n","\n","        return False"]},{"cell_type":"markdown","metadata":{},"source":["### Learning Rate Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class LRScheduler():\n","    def __init__(self):\n","        pass\n","    \n","    def step(self, current_lr, validation_loss):\n","        # Return the same learning rate\n","        return current_lr\n","\n","class ReduceLROnPlateau(LRScheduler):\n","    def __init__(self, decay_factor=0.5, patience=5, min_lr=0.0001, threshold=1e-4):\n","        # Save Parameters\n","        self.decay_factor = decay_factor\n","        self.patience = patience\n","        self.min_lr = min_lr\n","        self.threshold = threshold\n","        # Save past loss\n","        self.past_loss = []\n","\n","    def step(self, current_lr, validation_loss):\n","        # Append loss to past loss\n","        self.past_loss.append(validation_loss)\n","\n","        # Decay learning rate if no loss improved more than threshold% in the last patience steps\n","        ### TODO ###\n","\n","        return current_lr"]},{"cell_type":"markdown","metadata":{},"source":["### Utility Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Group iterable into size of n for mini-batch\n","# [1, 2, 3, 4, 5] -> n=2 -> [[1, 2], [3, 4], [5]]\n","def grouper(iterable, n):\n","    ### TODO ###\n","    return []"]},{"cell_type":"markdown","metadata":{},"source":["### The Main Training Loop\n","\n","This is where the whole training process comes together"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(model: SimpleNN,\n","                optimizer: Optimizer,\n","                lr_scheduler: LRScheduler,\n","                earlystop_scheduler: StopScheduler,\n","                training_data, validation_data,\n","                epochs: int = 100,\n","                mini_batch_size: int = 10,\n","                log_period: int = 10,\n","                draw_process: bool = False):\n","\n","    # Record loss and accuracy\n","    loss_his, acc_his  = [], []\n","    # If draw_process, store evaluation history and boundary table\n","    if draw_process:\n","        print(\"Draw process will take time to eval models and save boundary!\")\n","        print(\"Use larger log periods to save time!\")\n","        eval_his = []\n","\n","    # Run through the epochs\n","    progress_bar = tqdm(range(epochs))\n","    for epoch in progress_bar:\n","        # Create batches using grouper function and run through the batches\n","        for mini_batch in grouper(training_data, mini_batch_size):\n","            model.update_one_batch_optimizer(mini_batch, optimizer)\n","\n","        # Calculate loss and accuracy every counter epochs\n","        if epoch%log_period == 0:\n","            loss_his.append((model.loss(training_data), model.loss(validation_data)))\n","            te, ve = model.evaluate(training_data), model.evaluate(validation_data)\n","            acc_his.append((te[0], ve[0]))\n","            if draw_process:\n","                eval_his.append((calculate_model_boundary(model, te[1]), calculate_model_boundary(model, ve[1])))\n","\n","            # Update Progress Bar Description\n","            desc = f\"Train Loss: {loss_his[-1][0]:.3f}, Accuracy: {acc_his[-1][0]:.3f}\"\n","            progress_bar.set_description(desc)\n","            \n","            # Check if Early Stopping is needed using validation data\n","            if earlystop_scheduler.step(loss_his[-1][1]):\n","                print(\"Early Stopped!\")\n","                return (loss_his, acc_his, eval_his) if draw_process else (loss_his, acc_his)\n","                \n","            # Update learning rate according to validation loss \n","            optimizer.lr = lr_scheduler.step(optimizer.lr, loss_his[-1][1])\n","\n","    return (loss_his, acc_his, eval_his) if draw_process else (loss_his, acc_his)"]},{"cell_type":"markdown","metadata":{},"source":["### These are code use to draw the animations\n","\n","You can skip understanding them..."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_model_boundary(model: SimpleNN, evaluation_result):\n","    # Separate evaluation_result into buckets\n","    correct, error = [[], [], []], [[], [], []]\n","    for X, Y, pred in evaluation_result:\n","        bucket = correct if Y == pred else error\n","        bucket[0].append(X[0].item())\n","        bucket[1].append(X[1].item())\n","        bucket[2].append(Y)\n","    # Calculate model boundary limits\n","    x_min, x_max = min(correct[0]+error[0]), max(correct[0]+error[0])\n","    y_min, y_max = min(correct[1]+error[1]), max(correct[1]+error[1])\n","    step = max(x_max-x_min, y_max-y_min)/100\n","    xx, yy = np.meshgrid(np.arange(x_min-step*10, x_max+step*10, step), np.arange(y_min-step*10, y_max+step*10, step))\n","    # Evaluate mesh grid points on model\n","    z = np.array([model.evaluate_one(x.reshape(-1, 1)) for x in np.c_[xx.ravel(), yy.ravel()]]).reshape(xx.shape)\n","    return (correct, error, xx, yy, z)\n","\n","def draw_model_boundary(model_boundary, title: str = \"Metric\", prev_ax = None):\n","    correct, error, xx, yy, z = model_boundary\n","    # Init Plot if no ax\n","    if prev_ax is None:\n","        fig, ax = plt.subplots()\n","    else:\n","        ax = prev_ax\n","    cmap=plt.cm.Spectral\n","    # Plot boundary\n","    ax.contourf(xx, yy, z, alpha=0.4, cmap=cmap)\n","    # Plot Correct and Error Points from evaluation_result\n","    ax.scatter(correct[0], correct[1], c=correct[2], s=30, marker='o', label='correct', cmap=cmap)\n","    ax.scatter(error[0], error[1], c=error[2], s=30, marker='x', label='wrong', cmap=cmap)\n","    # Set the limit on the chart\n","    ax.set_xlim([xx.min(), xx.max()])\n","    ax.set_ylim([yy.min(), yy.max()])\n","    # Show legends\n","    ax.legend()\n","    if prev_ax is None:\n","        plt.show()\n","\n","def draw_metrics_history(metrics: List[Tuple[float, ...]], metric_names: Tuple[str, ...],\n","                         current_step: int, title: str = \"Metric\", prev_ax = None,\n","                         y_lim: Optional[List[float]] = None, tolog=False, ma_step=2):\n","    # Init Plot if no ax\n","    if prev_ax is None:\n","        fig, ax = plt.subplots()\n","    else:\n","        ax = prev_ax\n","    # Plot each of the lines\n","    assert len(metrics[0]) == len(metric_names), \"Each metric shall have a name!\"\n","    # Get data of each metric\n","    metric_datas = np.log(np.array(metrics).T) if tolog else np.array(metrics).T\n","    for metric_name, metric_data in zip(metric_names, metric_datas):\n","        # Calculate moving average for better plot\n","        mov_avg_metric = np.concatenate([metric_data[:ma_step-1], np.convolve(metric_data, np.ones(ma_step), 'valid') / ma_step])\n","        ax.plot(mov_avg_metric[:current_step+1], label=metric_name)\n","    # Set the limit on the chart\n","    ax.set_xlim([0, len(metrics)-1])\n","    if y_lim is not None:\n","        ax.set_ylim(y_lim)\n","    else:\n","        ax.set_ylim([metric_datas.min()-0.5, metric_datas.max()+0.5])\n","    # Set title\n","    ax.set_title(title)\n","    ax.legend()\n","    if prev_ax is None:\n","        plt.show()\n","\n","def draw_process(model, loss_his, acc_his, eval_his, train=False, interval=200):\n","    # Get the history of train or valid\n","    evaluate_data_id = 0 if train else 1\n","    evaluate_data = [his[evaluate_data_id] for his in eval_his]\n","    print(\"This will take some time to plot!\")\n","    print(f\"Total plots: {len(evaluate_data)}\")\n","    # Prepare the animation\n","    fig = plt.figure(constrained_layout=True)\n","    gs = gridspec.GridSpec(ncols=2, nrows=5, figure=fig)\n","    ax1 = fig.add_subplot(gs[:-1, :])\n","    ax2 = fig.add_subplot(gs[-1, 0])\n","    ax3 = fig.add_subplot(gs[-1, 1])\n","    def update(i):\n","        # Draw Model Boundary\n","        ax1.clear()\n","        ax1_title = f'Train Data Evaluation History: {i:04}' if train else f'Validation Data Evaluation History: {i:04}'\n","        draw_model_boundary(evaluate_data[i], ax1_title, prev_ax=ax1)\n","        # Draw Loss Change\n","        ax2.clear()\n","        ax2_title = \"Loss History (Y is log of loss)\"\n","        draw_metrics_history(loss_his, (\"Train\", \"Valid\"), i, ax2_title, prev_ax=ax2, ma_step=2, tolog=True)\n","        # Draw Accuracy Change\n","        ax3.clear()\n","        ax3_title = \"Accuracy History\"\n","        draw_metrics_history(acc_his, (\"Train\", \"Valid\"), i, ax3_title, prev_ax=ax3, ma_step=2, y_lim=[0, 1])\n","    anim = FuncAnimation(fig, update, frames=range(0, len(evaluate_data)), interval=interval, blit=False)\n","    return HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Run"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# Generate the dataset\n","class_cnt, training_data, validation_data, testing_data = gen_data(\"mul_circle\")\n","print(f\"Class Count: {class_cnt}\")\n","\n","# Setup the model structure\n","network = [2] + [4, 4] + [class_cnt]  # Input Dimension + Layers + Output Class Count\n","model = SimpleNN(network, AF.relu)\n","\n","# Setup the optimzer and schedulers\n","optimizer = SGD(model.empty_structure(), lr=0.01)\n","lr_scheduler = LRScheduler()\n","stop_scheduler = StopScheduler()\n","\n","# Run the training process\n","# loss_his, acc_his, eval_his = train_model(model, optimizer, lr_scheduler, stop_scheduler,\n","#                                           training_data, validation_data,\n","#                                           epochs=100, mini_batch_size=10, log_period=10, draw_process=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Draw the training process\n","# draw_process(model, loss_his, acc_his, eval_his)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Final evaluation on testing data\n","# accuracy, evaluation_result = model.evaluate(testing_data)\n","# print(\"Accuracy:\", accuracy)\n","# draw_model_boundary(calculate_model_boundary(model, evaluation_result))"]},{"cell_type":"markdown","metadata":{},"source":["### Experiments\n","\n","Do some experiments on the effect of different settings"]},{"cell_type":"markdown","metadata":{},"source":["#### Try out different daasets\n","- Must: \"checkboard\", \"spiral\", \"mul_circle\"\n","- Others: \"circle\", \"moon\", \"blob\""]},{"cell_type":"markdown","metadata":{},"source":["#### Try out different activation functions\n","- Must: AF.linear, AF.sigmoid, AF.tanh, AF.relu\n","- Others: AF.leaky_relu, AF.selu ..."]},{"cell_type":"markdown","metadata":{},"source":["#### Try out different model structures (deep vs shallow / wide vs thin)\n","- Must: (4)x1, (2)x4, (4)x2\n","- Others: (4)x3 ..."]},{"cell_type":"markdown","metadata":{},"source":["#### Try out different Gradient Descent Methods\n","- Must: Batch, Stochastic, Mini-batch with different batch size"]},{"cell_type":"markdown","metadata":{},"source":["#### Try out Different Optimizers\n","- Must: SGD, SGD+Momentum, RMSprop, Adam\n","- Others: SGD+NesterovMomentum, Adagrad"]},{"cell_type":"markdown","metadata":{},"source":["#### Must: Try out Early Stopping"]},{"cell_type":"markdown","metadata":{},"source":["#### Must: Try out Learning Rate Adjustment"]},{"cell_type":"markdown","metadata":{},"source":["#### Others: Try out different weight and bias initialization\n","- Eg. weights: Standard Normal, Normalized Standard Normal, Gaussian Normal\n","- Eg. bias: All zeros, Standard Normal, Gaussian Normal"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
